import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt

# Sigmoid activation function
def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

# Initialize parameters
def initialize_parameters(n_features):
    weights = np.zeros((n_features, 1))
    bias = 0
    return weights, bias

# Forward propagation
def forward_propagation(X, weights, bias):
    Z = np.dot(X, weights) + bias
    A = sigmoid(Z)
    return A

# Compute binary cross-entropy loss (cost)
def compute_cost(A, y):
    m = y.shape[0]
    cost = - (1/m) * np.sum(y * np.log(A + 1e-15) + (1 - y) * np.log(1 - A + 1e-15))
    return cost

# Backward propagation (gradient descent)
def backward_propagation(X, y, A, weights, bias, learning_rate):
    m = X.shape[0]
    dz = A - y
    dw = (1/m) * np.dot(X.T, dz)
    db = (1/m) * np.sum(dz)
    # Update parameters
    weights -= learning_rate * dw
    bias -= learning_rate * db
    return weights, bias

# Train the model
def train_model(X, y, learning_rate=0.01, num_iterations=1000):
    n_features = X.shape[1]
    weights, bias = initialize_parameters(n_features)
    costs = []
    
    for i in range(num_iterations):
        # Forward propagation
        A = forward_propagation(X, weights, bias)
        # Compute cost
        cost = compute_cost(A, y)
        costs.append(cost)
        # Backward propagation
        weights, bias = backward_propagation(X, y, A, weights, bias)
        # Print cost every 100 iterations
        if i % 100 == 0:
            print(f"Iteration {i}, Cost: {cost:.4f}")
    
    return weights, bias, costs

# Predict function
def predict(X, weights, bias):
    A = forward_propagation(X, weights, bias)
    return (A >= 0.5).astype(int)

# Main function
def main():
    # Step 1: Generate synthetic dataset (simulating Cats dataset)
    X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_clusters_per_class=1, random_state=42)
    y = y.reshape(-1, 1)  # Reshape y to (m, 1)
    
    # Split dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Step 2: Normalize the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # Step 3-6: Train the model
    learning_rate = 0.01
    num_iterations = 1000
    weights, bias, costs = train_model(X_train, y_train, learning_rate, num_iterations)
    
    # Step 7: Predict and evaluate
    y_pred_train = predict(X_train, weights, bias)
    y_pred_test = predict(X_test, weights, bias)
    
    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)
    conf_matrix = confusion_matrix(y_test, y_pred_test)
    
    print(f"\nTrain Accuracy: {train_accuracy:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")
    print("\nConfusion Matrix (Test Set):")
    print(conf_matrix)
    
    # Plot cost over iterations
    plt.plot(costs)
    plt.xlabel("Iterations")
    plt.ylabel("Cost")
    plt.title("Cost vs Iterations")
    plt.show()

if __name__ == "__main__":
    main()
